{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "results = pd.DataFrame(columns = ['model', 'task', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/wine+quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('winequality-red.csv', sep = ';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Разделите выборку на обучающую и тестовую в отношении 70%/30%, предварительно выделив целевую переменную (колонка 'quality')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Выделяем целевую переменную\n",
    "X = data.drop(columns=['quality'])  # признаки\n",
    "y = data['quality']  # целевая переменная\n",
    "\n",
    "# Разделение выборки на обучающую и тестовую\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Вывод размеров полученных выборок\n",
    "print(\"Размер обучающей выборки:\", X_train.shape, y_train.shape)\n",
    "print(\"Размер тестовой выборки:\", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "### Ваш код ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Оцените качество на тестовой выборке по метрике accuracy для классификаторов:\n",
    "\n",
    "DecisionTreeClassifier\n",
    "\n",
    "BaggingClassifier со 100 деревьями\n",
    "\n",
    "RandomForestClassifier со 100 деревьями\n",
    "\n",
    "Сравните результаты и напишите какой вывод можно сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Инициализация классификаторов\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "bagging_classifier = BaggingClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Обучение классификаторов на обучающей выборке\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на тестовой выборке\n",
    "dt_pred = dt_classifier.predict(X_test)\n",
    "bagging_pred = bagging_classifier.predict(X_test)\n",
    "rf_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Оценка качества на тестовой выборке\n",
    "acc_DT = accuracy_score(y_test, dt_pred)\n",
    "acc_Bagging = accuracy_score(y_test, bagging_pred)\n",
    "acc_RF = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "# acc_DT = 0\n",
    "# acc_Bagging = 0\n",
    "# acc_RF = 0\n",
    "results.loc[0] = ['DecisionTreeClassifier', 'task2', acc_DT]\n",
    "results.loc[1] = ['BaggingClassifier ', 'task2', acc_Bagging]\n",
    "results.loc[2] = ['RandomForestClassifier', 'task2', acc_RF]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Посчитайте качество на тестовой выборке по метрике accuracy для классификатора RandomForestClassifier, используя значения деревьев:\n",
    "    \n",
    "10, 50, 100, 200, далее с шагом 200 до 5000 деревьев.\n",
    "Постройте график зависимости качества от числа деревьев.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Задаем значения числа деревьев\n",
    "num_estimators = list(range(10, 5001, 200))\n",
    "\n",
    "# Инициализация списка для сохранения значений accuracy\n",
    "accuracy_scores = []\n",
    "\n",
    "# Обучение и оценка качества для каждого значения числа деревьев\n",
    "for n in num_estimators:\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    rf_pred = rf_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, rf_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Построение графика\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_estimators, accuracy_scores, marker='o', linestyle='-')\n",
    "plt.title('Зависимость качества RandomForestClassifier от числа деревьев')\n",
    "plt.xlabel('Число деревьев')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "### Ваш код ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Обучите реализации градиентного бустинга с параметрами по умолчанию из библиотек sklearn и xgboost. Сравните значение метрики accuracy по cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.where(y_train > 5, y_train - 6, y_train)\n",
    "\n",
    "y_test = np.where(y_test > 5, y_test - 6, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация классификаторов\n",
    "gbc_classifier = GradientBoostingClassifier(random_state=42)\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "# Оценка качества с помощью кросс-валидации для GradientBoostingClassifier\n",
    "acc_sklearn = np.mean(cross_val_score(gbc_classifier, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "# Оценка качества с помощью кросс-валидации для XGBClassifier\n",
    "acc_xgboost = np.mean(cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "# acc_sklearn = 0\n",
    "# acc_xgboost = 0\n",
    "results.loc[3] = ['GradientBoostingClassifier', 'task4', acc_sklearn]\n",
    "results.loc[4] = ['XGBClassifier', 'task4', acc_xgboost]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Подберите оптимальные параметры этих алгоритмов с помощью GridSearchCV(cv=3).\n",
    "Параметры для оптимизации:\n",
    "\n",
    "скорость обучения\n",
    "\n",
    "количество деревьев\n",
    "\n",
    "глубина деревьев\n",
    "\n",
    "Сравните значение метрики accuracy. Выведите лучшие параметры алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Параметры для GradientBoostingClassifier\n",
    "gbc_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Параметры для XGBClassifier\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# GridSearchCV для GradientBoostingClassifier\n",
    "gbc_grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42), gbc_param_grid, cv=3, scoring='accuracy')\n",
    "gbc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV для XGBClassifier\n",
    "xgb_grid_search = GridSearchCV(XGBClassifier(random_state=42), xgb_param_grid, cv=3, scoring='accuracy')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "best_params_sklearn = gbc_grid_search.best_params_\n",
    "print('Параметры лучшей модели sklearn:\\n', best_params_sklearn)\n",
    "best_params_xgboost = xgb_grid_search.best_params_\n",
    "print('Параметры лучшей модели xgboost:\\n', best_params_xgboost)\n",
    "acc_sklearn_cv = xgb_grid_search.best_score_\n",
    "acc_xgboost_cv = gbc_grid_search.best_score_\n",
    "results.loc[5] = ['GradientBoostingClassifier_CV', 'task5', acc_sklearn_cv]\n",
    "results.loc[6] = ['XGBClassifier_CV', 'task5', acc_xgboost_cv]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Обучите реализации градиентного бустинга с параметрами по умолчанию из библиотек lightgbm и catboost. Сравните значение метрики accuracy по cross_val_score по всем четырем реализациям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Инициализация классификаторов\n",
    "lgbm_classifier = LGBMClassifier(random_state=42)\n",
    "catboost_classifier = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "# Оценка качества с помощью кросс-валидации для GradientBoostingClassifier\n",
    "acc_lightgbm = cross_val_score(lgbm_classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Оценка качества с помощью кросс-валидации для XGBClassifier\n",
    "acc_catboost = cross_val_score(catboost_classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Среднее значение accuracy для LightGBM:\", np.mean(acc_lightgbm))\n",
    "print(\"Среднее значение accuracy для CatBoost:\", np.mean(acc_lightgbm))\n",
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "results.loc[7] = ['LGBMClassifier', 'task6', acc_lightgbm]\n",
    "results.loc[8] = ['CatBoostClassifier', 'task6', acc_catboost]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Подберите оптимальные параметры для алгоритмов градиентного бустинга из библиотек lightgbm и catboost с теми же условиями.\n",
    "Сравните значение метрики accuracy. Выведите лучшие параметры алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "\n",
    "# Параметры для LightGBM\n",
    "lgbm_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Параметры для CatBoost\n",
    "catboost_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# GridSearchCV для LightGBM\n",
    "lgbm_grid_search = GridSearchCV(LGBMClassifier(random_state=42), lgbm_param_grid, cv=3, scoring='accuracy')\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV для CatBoost\n",
    "catboost_grid_search = GridSearchCV(CatBoostClassifier(random_state=42, silent=True), catboost_param_grid, cv=3, scoring='accuracy')\n",
    "catboost_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_params_lightgbm = lgbm_grid_search.best_params_\n",
    "print('Параметры лучшей модели lightgbm:\\n', best_params_lightgbm)\n",
    "best_params_catboost = catboost_grid_search.best_params_\n",
    "print('Параметры лучшей модели catboost:\\n', best_params_catboost)\n",
    "acc_lightgbm_cv = lgbm_grid_search.best_score_\n",
    "acc_catboost_cv = catboost_grid_search.best_score_\n",
    "results.loc[9] = ['LGBMClassifier_CV', 'task7', acc_lightgbm_cv]\n",
    "results.loc[10] = ['CatBoostClassifier_CV', 'task7', acc_catboost_cv]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Подберите оптимальные параметры алгоритма из библиотеки xgbost с помощью [optuna](https://github.com/optuna/optuna) . Параметры для оптимизации:\n",
    "\n",
    "скорость обучения\n",
    "\n",
    "количество деревьев\n",
    "\n",
    "глубина деревьев\n",
    "\n",
    "Сравните результат с поиском по сетке из sklearn. Выведите лучшие параметры алгоритма, найденные даным способом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Ваш код ###\n",
    "\n",
    "\n",
    "def test(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.1, 0.5]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7)\n",
    "    }\n",
    "    \n",
    "    xgb_classifier = XGBClassifier(random_state=42, **params)\n",
    "    score = cross_val_score(xgb_classifier, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(test, n_trials=100)\n",
    "\n",
    "best_params_optuna = study.best_params\n",
    "best_score_optuna = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры XGBoost, найденные с помощью Optuna:\", best_params_optuna)\n",
    "print(\"Лучшее значение accuracy, найденное с помощью Optuna:\", best_score_optuna)\n",
    "\n",
    "# Сравнение с поиском по сетке из scikit-learn\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(XGBClassifier(random_state=42), xgb_param_grid, cv=3, scoring='accuracy')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_xgboost_optuna = xgb_grid_search.best_params_\n",
    "acc_xgboost_optuna = xgb_grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "print('Параметры лучшей модели xgboost:\\n', best_params_xgboost_optuna)\n",
    "results.loc[11] = ['XGBClassifier_optuna', 'task8', acc_xgboost_optuna]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Выведите качество по метрике accuracy стэкинга (StackingClassifier) 4-х алгоритмов с базовыми параметрами градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Ваш код ###\n",
    "\n",
    "\n",
    "# Инициализация базовых моделей\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(random_state=42)),\n",
    "    ('gradient_boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('ada_boost', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Инициализация мета-классификатора\n",
    "meta_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Инициализация StackingClassifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=meta_classifier)\n",
    "\n",
    "# Оценка качества с помощью кросс-валидации для StackingClassifier\n",
    "stacking_accuracy_scores = cross_val_score(stacking_classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "acc_stacking_default = stacking_accuracy_scores.mean()\n",
    "results.loc[12] = ['Stacking', 'task10', acc_stacking_default]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Выведите качество по метрике accuracy стэкинга 4-х алгоритмов с оптимальными параметрами градиентного бустинга. Сравните результаты с предыдущим шагом и напишите какой вывод можно из этого сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
